{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nimori/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "/home/nimori/venv_envs/tf24/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin'\n",
    "google_word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories={'food':['ration', 'shop', 'community', 'kitchen'], 'jobs':['training', 'professional', 'job'], \n",
    "           'money':['invest','save','bank','donation'],\n",
    "           'utilities':['internet', 'phone', 'electricity', 'water', 'landlord', 'hotel', 'shelter', 'lpg', 'waste'],\n",
    "           'medical':['hospitals', 'facilities', 'specialists', 'blood'],\n",
    "           'education':['school', 'college', 'tuitions', 'career', 'consultations'],\n",
    "           'medical':['hospitals', 'facilities', 'specialists', 'blood'], 'security':['police', 'theft', 'army', 'guard'],\n",
    "           'infrastructure':['road', 'bridge', 'sewage', 'traffic'],\n",
    "           'buy':['shopkeeper', 'land', 'apartment', 'furniture', 'electronics', 'rental'],\n",
    "           'sell':['shopkeeper', 'land', 'apartment', 'furniture', 'electronics', 'rental'],\n",
    "           'government':['schemes', 'corruption'], 'politics':['politics'],\n",
    "           'emergency':['covid', 'blood', 'robbery', 'crime'],\n",
    "           'travel':['transport', 'cab', 'public', 'auto', 'hotel', 'traffic', 'tourism', 'tolls'],\n",
    "           'services':['business', 'legal', 'accountant', 'carpenter', 'mechanic', 'electrician', 'plumber', 'house', 'help', 'labour'],\n",
    "           'other':['parking', 'women', 'human', 'rights', 'consumer', 'sanitation'], 'technology':['technology'], 'environment':['environment', 'animals']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent= pk.load(open('translated.pk', 'rb'))\n",
    "#sent would be a list in which each element is another list that comprises of the following 3 things:\n",
    "##1) an ID\n",
    "##2) original text\n",
    "##3) translated text (in the form of another list)\n",
    "                            \n",
    "                             ######Example shown below########\n",
    "###[1601345851000,'मुख्यमंत्री गहलोत बोले-प्रदेश में जब भी कोई नया जिला बनेगा तो सबसे पहले ब्यावर का होगा नाम',\n",
    "###  ['Chief Minister Gehlot said - whenever a new district is formed in the state, Beawar will be named first.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= []\n",
    "for i in range(len(sent)):\n",
    "    text.append(sent[i][2]) #to select just the translated text\n",
    "#text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nimori/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "def get_unigram(text):\n",
    "    '''\n",
    "    preprocessing: tokenization; stemming\n",
    "    '''\n",
    "    tt=[]\n",
    "    for j in text:\n",
    "        j=re.sub(r\"\\W\", \" \",j, flags=re.I).strip()\n",
    "        \n",
    "        for i in j.split():\n",
    "#             i=ps.stem(i.lower())\n",
    "            i=i.lower()\n",
    "            \n",
    "            if i.isdigit() or len(i)<=2:\n",
    "                continue\n",
    "            \n",
    "            if i in stopwords.words('english'):\n",
    "                continue\n",
    "            else:\n",
    "                tt.append(i)\n",
    "    return list(set(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text2= []\n",
    "for txt in text:\n",
    "    sentence= ' '.join([str(elem) for elem in txt])\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    sent_final= ' '.join(edited_sentence)\n",
    "    text2.append([sent_final])\n",
    "    #print(' '.join(edited_sentence))\n",
    "#text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens= []\n",
    "for txt in text2:\n",
    "    tokens.append(get_unigram(txt))\n",
    "#tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Cosine Similarity using Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POC for first token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment\n",
      "animals\n",
      "covid\n",
      "blood\n",
      "robbery\n",
      "crime\n",
      "technology\n",
      "technology\n",
      "schemes\n",
      "corruption\n",
      "school\n",
      "college\n",
      "tuitions\n",
      "career\n",
      "consultations\n",
      "parking\n",
      "women\n",
      "human\n",
      "rights\n",
      "consumer\n",
      "sanitation\n"
     ]
    }
   ],
   "source": [
    "tk={}\n",
    "for i in tokens[0]:\n",
    "\n",
    "        vec1=google_word2vec[i]\n",
    "        cat = {}\n",
    "        for j in categories.keys():\n",
    "            vec2=google_word2vec[j]\n",
    "            sim = cosine_similarity(vec1.reshape(1, -1),vec2.reshape(1, -1))\n",
    "            if sim>0.1:\n",
    "                cat[j]=sim[0][0]\n",
    "        categ=sorted(cat.items(),key=lambda x:x[1])[::-1]\n",
    "        if categ!=[]:\n",
    "            for k in categories[categ[0][0]]:\n",
    "                print(k)\n",
    "            tk[i]=categ[0][0]\n",
    "#tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat= list(categories.keys())\n",
    "#cat[0]\n",
    "#tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'environment': [[0.08332595229148865]], 'animals': [[0.08332595229148865]]},\n",
       " {'covid': [[0.20476628839969635]],\n",
       "  'blood': [[0.20476628839969635]],\n",
       "  'robbery': [[0.20476628839969635]],\n",
       "  'crime': [[0.20476628839969635]]},\n",
       " {'technology': [[0.0332690067589283]]},\n",
       " {'technology': [[0.0332690067589283]]},\n",
       " {'schemes': [[0.09642307460308075]], 'corruption': [[0.09642307460308075]]},\n",
       " {'school': [[0.0464131236076355]],\n",
       "  'college': [[0.0464131236076355]],\n",
       "  'tuitions': [[0.0464131236076355]],\n",
       "  'career': [[0.0464131236076355]],\n",
       "  'consultations': [[0.0464131236076355]]},\n",
       " {'parking': [[0.2472761571407318]],\n",
       "  'women': [[0.2472761571407318]],\n",
       "  'human': [[0.2472761571407318]],\n",
       "  'rights': [[0.2472761571407318]],\n",
       "  'consumer': [[0.2472761571407318]],\n",
       "  'sanitation': [[0.2472761571407318]]}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col= []\n",
    "for cat in tk.values():\n",
    "    vec1=google_word2vec[cat]\n",
    "    for token in tk.keys():\n",
    "        vec2=google_word2vec[token]\n",
    "        row= {}\n",
    "        for subcat in categories.get(cat):\n",
    "            row[subcat]= cosine_similarity(vec1.reshape(1, -1),vec2.reshape(1, -1)).tolist()\n",
    "    col.append(row)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_0= []\n",
    "li= []\n",
    "for dic in col:\n",
    "    li_0.append(list(dic.values()))\n",
    "    li.append(li_0[0][0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
